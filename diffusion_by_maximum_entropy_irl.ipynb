{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juihuichung/dxmi/blob/main/diffusion_by_maximum_entropy_irl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed                     = 42          # Random seed for reproducibility\n",
        "\n",
        "data_size                = (1, 8, 8)   # Shape of input data: (channels, height, width)\n",
        "limit_samples            = 20000       # Maximum number of samples used during training\n",
        "label                    = None        # Set to 0,1,...9 or None (include all numbers)\n",
        "train_batch_size         = 128         # Number of samples per training batch\n",
        "\n",
        "diffusion_num_timesteps  = 1000        # Total number of timesteps in reverse process and image generation\n",
        "diffusion_num_epochs     = 10          # Number of epochs for training the diffusion model\n",
        "diffusion_learning_rate  = 1e-3        # Learning rate for the diffusion model optimizer\n",
        "diffusion_embedding_size = 100         # Dimensionality of input embeddings in the diffusion model\n",
        "diffusion_hidden_size    = 512         # Hidden layer size in diffusion model (policy network) MLP\n",
        "diffusion_hidden_layers  = 5           # Number of hidden layers in the diffusion model\n",
        "\n",
        "n_reduced_timesteps      = 5           # The reduced timesteps in DxMI\n",
        "hidden_size_value        = 512         # Hidden layer size in the value network\n",
        "hidden_layers_value      = 5           # Number of hidden layers in the value network\n",
        "learning_rate_policy     = 1e-7        # Learning rate for policy network\n",
        "learning_rate_value      = 1e-6        # Learning rate for value network\n",
        "\n",
        "dxmi_num_epochs          = 10          # Number of epochs for the DxMI\n",
        "dxmi_target_network      = False       # Whether to use a target network for stabilization\n",
        "dxmi_entropy_coef        = 0.0         # Entropy regularization coefficient (encourages exploration)\n",
        "dxmi_cost_coef           = 0.1         # Coefficient for the cost/loss term associated with predicted values\n",
        "dxmi_gradient_clipping   = 0.0         # Gradient clipping threshold (0.0 means no clipping)\n"
      ],
      "metadata": {
        "id": "tZGnde-5RNKy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ggrS7xnHZUE",
        "outputId": "794449ba-b8d7-4527-9f96-b4e6a44acc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics[image] scipy torch-fidelity  > /dev/null 2>&1\n",
        "!pip install numpy==1.26.4\n",
        "# restart runtime after this, since the VAR_sampling requires numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZddMW-WZnY",
        "outputId": "75a10e1c-2930-47b5-a238-34f064dca0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... packages loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dc88c4f2430>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import MNIST\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "print(\"... packages loaded\", flush=True)\n",
        "\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lpO_WIIrTCID"
      },
      "outputs": [],
      "source": [
        "def mnist_dataset(train=True, limit_samples=None, data_size=(1, 8, 8), label=None):\n",
        "    from torchvision.datasets import MNIST\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    train_dataset = MNIST(\"./data\", download=True, train=train)\n",
        "\n",
        "    data = train_dataset.data.float() / 255.0\n",
        "    data = (data * 2.0) - 1.0\n",
        "\n",
        "    target = train_dataset.targets\n",
        "\n",
        "    data = data.unsqueeze(1)\n",
        "\n",
        "    target_h, target_w = data_size[1], data_size[2]\n",
        "    data = F.interpolate(data, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "\n",
        "    if label is not None:\n",
        "        mask = (target == label)\n",
        "        data = data[mask]\n",
        "\n",
        "    if limit_samples is not None:\n",
        "        data = data[:limit_samples]\n",
        "        target = target[:limit_samples]\n",
        "\n",
        "    return data, target\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, size: int):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(size)\n",
        "        self.ff = nn.Linear(size, size)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x + self.act(self.ff(self.ln(x)))\n",
        "\n",
        "\n",
        "class SinusoidalEmbedding(nn.Module):\n",
        "    def __init__(self, size: int, scale: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.scale = scale\n",
        "        half_size = self.size // 2\n",
        "        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n",
        "        emb = torch.exp(-emb * torch.arange(half_size))\n",
        "        self.emb = nn.Parameter(emb, requires_grad=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x * self.scale\n",
        "        emb = x * self.emb.unsqueeze(0)\n",
        "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128, data_size: tuple = (1, 28, 28)):\n",
        "        super().__init__()\n",
        "        self.time_emb = SinusoidalEmbedding(emb_size)\n",
        "\n",
        "        flattened_dim = data_size[0] * data_size[1] * data_size[2]\n",
        "        self.concat_size = flattened_dim + emb_size  # concatenating image vector and time embedding\n",
        "        self.data_dim = flattened_dim\n",
        "\n",
        "        layers = [nn.Linear(self.concat_size, hidden_size), nn.GELU()]\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(Block(hidden_size))\n",
        "        layers.append(nn.LayerNorm(hidden_size))\n",
        "        layers.append(nn.Linear(hidden_size, self.data_dim))\n",
        "        self.joint_mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        original_shape = x.shape\n",
        "        x_flat = x.reshape(x.shape[0], -1)\n",
        "        t_emb = self.time_emb(t.reshape(-1, 1))\n",
        "        x_cat = torch.cat((x_flat, t_emb), dim=-1)\n",
        "        out = self.joint_mlp(x_cat)\n",
        "        return out.reshape(original_shape)\n",
        "\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, data_size: tuple = (1, 28, 28)):\n",
        "        super().__init__()\n",
        "        flattened_dim = data_size[0] * data_size[1] * data_size[2]\n",
        "        layers = [nn.Linear(flattened_dim, hidden_size), nn.GELU()]\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(Block(hidden_size))\n",
        "        layers.append(nn.LayerNorm(hidden_size))\n",
        "        layers.append(nn.Linear(hidden_size, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_flat = x.reshape(x.shape[0], -1)\n",
        "        return self.net(x_flat)\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    # adapted from https://github.com/schinger/DiffusionModel\n",
        "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
        "        super().__init__()\n",
        "        self.num_timesteps = num_timesteps\n",
        "\n",
        "        betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
        "        sqrt_alphas_cumprod = alphas_cumprod ** 0.5\n",
        "        sqrt_one_minus_alphas_cumprod = (1 - alphas_cumprod) ** 0.5\n",
        "        sqrt_inv_alphas = torch.sqrt(1.0 / alphas)\n",
        "        noise_coef = betas / sqrt_one_minus_alphas_cumprod\n",
        "        variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "        buffers = {\n",
        "            'betas': betas,\n",
        "            'alphas': alphas,\n",
        "            'alphas_cumprod': alphas_cumprod,\n",
        "            'alphas_cumprod_prev': alphas_cumprod_prev,\n",
        "            'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
        "            'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod,\n",
        "            'sqrt_inv_alphas': sqrt_inv_alphas,\n",
        "            'noise_coef': noise_coef,\n",
        "            'variance': variance\n",
        "        }\n",
        "\n",
        "        for name, tensor in buffers.items():\n",
        "            self.register_buffer(name, tensor)\n",
        "\n",
        "    def add_noise(self, x_start, x_noise, timesteps):\n",
        "        s1 = self.sqrt_alphas_cumprod[timesteps].reshape(-1, 1, 1, 1)\n",
        "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps].reshape(-1, 1, 1, 1)\n",
        "        result = s1 * x_start + s2 * x_noise\n",
        "        return result\n",
        "\n",
        "    def sample_step(self, model_output, timestep, sample):\n",
        "        s1 = self.sqrt_inv_alphas[timestep].reshape(-1, 1, 1, 1)\n",
        "        s2 = self.noise_coef[timestep].reshape(-1, 1, 1, 1)\n",
        "        s3 = self.variance[timestep].reshape(-1, 1, 1, 1) ** 0.5\n",
        "\n",
        "        noise = torch.randn_like(model_output)\n",
        "        mask = (timestep == 0).reshape(-1, 1, 1, 1).expand_as(model_output)\n",
        "        noise = torch.where(mask, torch.zeros_like(noise), noise)\n",
        "\n",
        "        result = s1 * (sample - s2 * model_output) + s3 * noise\n",
        "        return result\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_timesteps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PmOqyCSij0gQ"
      },
      "outputs": [],
      "source": [
        "def sample_images(policy_net, diffusion, device, sample_size=10):\n",
        "    policy_net.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start from pure Gaussian noise\n",
        "        samples = torch.randn(sample_size, *data_size, device=device)\n",
        "        # Reverse diffusion loop: from timestep num_timesteps-1 down to 0\n",
        "        for t in reversed(range(num_timesteps)):\n",
        "            t_tensor = torch.full((sample_size,), t, device=device, dtype=torch.float32)\n",
        "            policy_output = policy_net(samples, t_tensor)\n",
        "            samples = diffusion.sample_step(policy_output, t_tensor.long(), samples)\n",
        "        samples = torch.clamp(samples, -1, 1)\n",
        "        # samples = ((samples + 1) / 2 * 255).to(torch.uint8)\n",
        "\n",
        "    return samples # claim to [-1,1]\n",
        "\n",
        "\n",
        "def compute_fid(policy_net, diffusion, device, sample_size=100, feature=192, sampler=None):\n",
        "    fid = FrechetInceptionDistance(feature=feature, normalize=True).to(device)\n",
        "\n",
        "    def preprocess(x):\n",
        "        x = x.repeat(1,3,1,1)\n",
        "        x = F.interpolate(x, size=(299,299), mode='bilinear', align_corners=False)\n",
        "        return (x + 1.0)/2.0\n",
        "\n",
        "\n",
        "    real = train_data[:sample_size].to(device)\n",
        "    if sampler is not None:\n",
        "        # use var_sampler\n",
        "        sampler.eval()\n",
        "        d_sample = sampler.sample(sample_size, device=device)\n",
        "        fake = torch.clamp(d_sample['sample'],-1,1)\n",
        "    else:\n",
        "        fake = sample_images(policy_net, diffusion, device, sample_size=sample_size)\n",
        "    # fake = torch.rand_like(real).mul(2).sub(1).to(device)\n",
        "\n",
        "    fake = torch.clamp(fake, -1,1) # just in case\n",
        "\n",
        "\n",
        "    real_loader = DataLoader(TensorDataset(real), batch_size=128, shuffle=False)\n",
        "    fake_loader = DataLoader(TensorDataset(fake), batch_size=128, shuffle=False)\n",
        "\n",
        "    for (batch,) in real_loader:\n",
        "        fid.update(preprocess(batch.to(device)), real=True)\n",
        "\n",
        "    for (batch,) in fake_loader:\n",
        "        fid.update(preprocess(batch.to(device)), real=False)\n",
        "\n",
        "    return fid.compute().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV97YIWkBD8D",
        "outputId": "b26b42f6-5b80-4e96-937c-bcdd2b2d706e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... 0511_1300\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "now = datetime.datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "now_str = now.strftime(\"%m%d_%H%M\")\n",
        "print(\"...\", now_str)\n",
        "\n",
        "# check_file = now.strftime(\"dxmi_checkpoint_%m%d_%H%M.pth\")\n",
        "# eval_file = now.strftime(\"dxmi_evaluation_results_%m%d_%H%M.txt\")\n",
        "\n",
        "diffusion_file = now.strftime(f\"pretraining_diffusion_results_{now_str}.txt\")\n",
        "log_file       = now.strftime(\"log_dxmi_%m%d_%H%M.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzhp_0UWXjcw",
        "outputId": "017dde0f-8800-4fbe-b424-c82e80e63d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 493kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.56MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.51MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training data: 20000\n"
          ]
        }
      ],
      "source": [
        "# data_size = (1, 8, 8)\n",
        "# limit_samples = 20000\n",
        "# label = None\n",
        "# train_batch_size = 128\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    \"data_size\":   data_size,\n",
        "    \"limit_samples\": limit_samples,\n",
        "    \"label\": label,\n",
        "    \"train_batch_size\": train_batch_size,\n",
        "    \"seed\": seed\n",
        "}\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"=== Data info ===\\n\")\n",
        "    for name, val in hyperparams.items():\n",
        "        f.write(f\"{name}: {val}\\n\")\n",
        "    f.write(\"\\n\")   # blank line for separation\n",
        "    f.flush()       # ensure it's written out immediately\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "train_data, target_data = mnist_dataset(train=True, limit_samples=limit_samples, data_size=data_size, label=label)\n",
        "train_dataset = TensorDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "print(\"Length of training data:\", len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GqoHv7lWcbk",
        "outputId": "e0c72a9c-7cd8-45d8-ecb9-e7bfcf58ce92",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.266253, FID: 9.894085\n",
            "Epoch 2/10, Loss: 0.148611\n",
            "Epoch 3/10, Loss: 0.134554\n",
            "Epoch 4/10, Loss: 0.129400\n",
            "Epoch 5/10, Loss: 0.126885\n",
            "Epoch 6/10, Loss: 0.121043\n",
            "Epoch 7/10, Loss: 0.113898\n",
            "Epoch 8/10, Loss: 0.112428\n",
            "Epoch 9/10, Loss: 0.112234\n",
            "Epoch 10/10, Loss: 0.109794, FID: 1.115394\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "# learning_rate = 1e-3\n",
        "# num_epochs = 500\n",
        "# num_timesteps = 1000\n",
        "# embedding_size = 100\n",
        "# hidden_size = 512\n",
        "# hidden_layers = 5\n",
        "\n",
        "\n",
        "learning_rate = diffusion_learning_rate\n",
        "num_epochs = diffusion_num_epochs\n",
        "num_timesteps = diffusion_num_timesteps\n",
        "embedding_size = diffusion_embedding_size\n",
        "hidden_size = diffusion_hidden_size\n",
        "hidden_layers = diffusion_hidden_layers\n",
        "\n",
        "hyperparams = {\n",
        "    \"learning_rate\":    learning_rate,\n",
        "    \"num_epochs\":      num_epochs,\n",
        "    \"num_timesteps\":   num_timesteps,\n",
        "    \"embedding_size\":  embedding_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"hidden_layers\": hidden_layers,\n",
        "    \"device\": str(device)\n",
        "}\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"=== Hyperparameters for diffusion model ===\\n\")\n",
        "    for name, val in hyperparams.items():\n",
        "        f.write(f\"{name}: {val}\\n\")\n",
        "    f.write(\"\\n\")   # blank line for separation\n",
        "    f.flush()       # ensure it's written out immediately\n",
        "\n",
        "\n",
        "\n",
        "policy_net = PolicyNet(hidden_size=hidden_size, hidden_layers=hidden_layers,\n",
        "            emb_size=embedding_size, data_size=data_size).to(device)\n",
        "diffusion = Diffusion(num_timesteps=num_timesteps).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    policy_net.train()\n",
        "    epoch_loss = 0.0\n",
        "    for (batch_data,) in train_loader:\n",
        "        x = batch_data.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        t = torch.randint(0, num_timesteps, (x.size(0),), device=device)\n",
        "        noise = torch.randn_like(x)\n",
        "        x_noisy = diffusion.add_noise(x, noise, t)\n",
        "        predicted_noise = policy_net(x_noisy, t.float())\n",
        "        loss = F.mse_loss(predicted_noise, noise)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    output_line = f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\"\n",
        "    if epoch == 0 or (epoch+1) % 10 == 0 or (epoch+1) == num_epochs:\n",
        "        score = compute_fid(policy_net, diffusion, device)\n",
        "        output_line += f\", FID: {score:.6f}\"\n",
        "    print(output_line)\n",
        "    with open(diffusion_file, 'a') as f:\n",
        "        f.write(output_line + '\\n')\n",
        "        f.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "-vqojMCpkYyU",
        "outputId": "14ff4975-12ce-4be9-fa8a-c6fe8470a017"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Generate 10 samples.\n",
        "samples = sample_images(policy_net, diffusion, device, sample_size=10)\n",
        "\n",
        "# Set up a 2x5 grid for 10 images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, ax in enumerate(axes):\n",
        "    img = samples[idx]\n",
        "    img = torch.clamp(img, -1, 1)\n",
        "    img = ((img + 1) / 2 * 255).to(torch.uint8)\n",
        "    img = img.permute(1, 2, 0)\n",
        "    ax.imshow(img.cpu().numpy(), cmap='gray')\n",
        "    ax.axis('off')  # Remove axes ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'pretrained_images_{now_str}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eigpEBubaY7N",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "adopted and refactored from\n",
        "https://github.com/UW-Madison-Lee-Lab/SFT-PG/blob/main/finetune.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Normal\n",
        "\n",
        "unsqueeze3x = lambda x: x[..., None, None, None]\n",
        "\n",
        "diffusion_config = {\n",
        "    \"beta_0\": 0.0001,\n",
        "    \"beta_T\": 0.02,\n",
        "    \"T\": 1000,\n",
        "}\n",
        "\n",
        "\n",
        "def process_single_t(x, t):\n",
        "    \"\"\"make single integer t into a vector of an appropriate size\"\"\"\n",
        "    if isinstance(t, int) or len(t.shape) == 0 or len(t) == 1:\n",
        "        t = torch.ones([x.shape[0]], dtype=torch.long, device=x.device) * t\n",
        "    return t\n",
        "\n",
        "def calc_diffusion_hyperparams(T, beta_0, beta_T):\n",
        "    \"\"\"\n",
        "    Compute diffusion process hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    T (int):                    number of diffusion steps\n",
        "    beta_0 and beta_T (float):  beta schedule start/end value,\n",
        "                                where any beta_t in the middle is linearly interpolated\n",
        "\n",
        "    Returns:\n",
        "    a dictionary of diffusion hyperparameters including:\n",
        "        T (int), Beta/Alpha/Alpha_bar/Sigma (torch.tensor on cpu, shape=(T, ))\n",
        "    \"\"\"\n",
        "\n",
        "    Beta = torch.linspace(beta_0, beta_T, T)\n",
        "    Alpha = 1 - Beta\n",
        "    Alpha_bar = Alpha + 0\n",
        "    Beta_tilde = Beta + 0\n",
        "    for t in range(1, T):\n",
        "        Alpha_bar[t] *= Alpha_bar[t-1]\n",
        "        Beta_tilde[t] *= (1-Alpha_bar[t-1]) / (1-Alpha_bar[t])\n",
        "    Sigma = torch.sqrt(Beta_tilde)\n",
        "\n",
        "    _dh = {}\n",
        "    _dh[\"T\"], _dh[\"Beta\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Sigma\"] = T, Beta, Alpha, Alpha_bar, Sigma\n",
        "    diffusion_hyperparams = _dh\n",
        "    return diffusion_hyperparams\n",
        "\n",
        "def bisearch(f, domain, target, eps=1e-8):\n",
        "    \"\"\"\n",
        "    find smallest x such that f(x) > target\n",
        "\n",
        "    Parameters:\n",
        "    f (function):               function\n",
        "    domain (tuple):             x in (left, right)\n",
        "    target (float):             target value\n",
        "\n",
        "    Returns:\n",
        "    x (float)\n",
        "    \"\"\"\n",
        "    #\n",
        "    sign = -1 if target < 0 else 1\n",
        "    left, right = domain\n",
        "    for _ in range(1000):\n",
        "        x = (left + right) / 2\n",
        "        if f(x) < target:\n",
        "            right = x\n",
        "        elif f(x) > (1 + sign * eps) * target:\n",
        "            left = x\n",
        "        else:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_VAR_noise(S, schedule='linear'):\n",
        "    \"\"\"\n",
        "    Compute VAR noise levels\n",
        "\n",
        "    Parameters:\n",
        "    S (int):            approximante diffusion process length\n",
        "    schedule (str):     linear or quadratic\n",
        "\n",
        "    Returns:\n",
        "    np array of noise levels, size = (S, )\n",
        "    \"\"\"\n",
        "    target = np.prod(1 - np.linspace(diffusion_config[\"beta_0\"], diffusion_config[\"beta_T\"], diffusion_config[\"T\"])) # target = alpha_T_bar\n",
        "\n",
        "    if schedule == 'linear':\n",
        "        g = lambda x: np.linspace(diffusion_config[\"beta_0\"], x, S)\n",
        "        domain = (diffusion_config[\"beta_0\"], 0.99)\n",
        "    elif schedule == 'quadratic':\n",
        "        g = lambda x: np.array([diffusion_config[\"beta_0\"] * (1+i*x) ** 2 for i in range(S)])\n",
        "        domain = (0.0, 0.95 / np.sqrt(diffusion_config[\"beta_0\"]) / S)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    f = lambda x: np.prod(1 - g(x))\n",
        "    largest_var = bisearch(f, domain, target, eps=1e-4)\n",
        "    return g(largest_var)\n",
        "\n",
        "\n",
        "def _log_gamma(x):\n",
        "    # Gamma(x+1) ~= sqrt(2\\pi x) * (x/e)^x  (1 + 1 / 12x)\n",
        "    y = x - 1\n",
        "    return np.log(2 * np.pi * y) / 2 + y * (np.log(y) - 1) + np.log(1 + 1 / (12 * y))\n",
        "\n",
        "\n",
        "def _log_cont_noise(t, beta_0, beta_T, T):\n",
        "    # We want log_cont_noise(t, beta_0, beta_T, T) ~= np.log(Alpha_bar[-1].numpy())\n",
        "    delta_beta = (beta_T - beta_0) / (T - 1)\n",
        "    _c = (1.0 - beta_0) / delta_beta\n",
        "    t_1 = t + 1\n",
        "    return t_1 * np.log(delta_beta) + _log_gamma(_c + 1) - _log_gamma(_c - t_1 + 1)\n",
        "\n",
        "\n",
        "# VAR\n",
        "def _precompute_VAR_steps(diffusion_hyperparams, user_defined_eta, device=None):\n",
        "    _dh = diffusion_hyperparams\n",
        "    T, Alpha, Alpha_bar, Beta = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Beta\"]\n",
        "    assert len(Alpha_bar) == T\n",
        "\n",
        "    # compute diffusion hyperparameters for user defined noise\n",
        "    T_user = len(user_defined_eta)\n",
        "    Beta_tilde = torch.from_numpy(user_defined_eta).to(torch.float32).to(device)\n",
        "\n",
        "    Gamma_bar = 1 - Beta_tilde\n",
        "    for t in range(1, T_user):\n",
        "        Gamma_bar[t] *= Gamma_bar[t-1]\n",
        "\n",
        "    assert Gamma_bar[0] <= Alpha_bar[0] and Gamma_bar[-1] >= Alpha_bar[-1]\n",
        "\n",
        "    continuous_steps = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T_user-1, -1, -1):\n",
        "            t_adapted = None\n",
        "            for i in range(T - 1):\n",
        "                if Alpha_bar[i] >= Gamma_bar[t] > Alpha_bar[i+1]:\n",
        "                    t_adapted = bisearch(f=lambda _t: _log_cont_noise(_t, Beta[0].cpu().numpy(), Beta[-1].cpu().numpy(), T),\n",
        "                                            domain=(i-0.01, i+1.01),\n",
        "                                            target=np.log(Gamma_bar[t].cpu().numpy()))\n",
        "                    break\n",
        "            if t_adapted is None:\n",
        "                t_adapted = T - 1\n",
        "            continuous_steps.append(t_adapted)  # must be decreasing\n",
        "    return continuous_steps\n",
        "\n",
        "\n",
        "def VAR_get_params(diffusion_hyperparams, user_defined_eta, kappa, continuous_steps):\n",
        "    \"\"\"modified to remove map_gpu\"\"\"\n",
        "\n",
        "    _dh = diffusion_hyperparams\n",
        "    T, Alpha, Alpha_bar, Beta = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Beta\"]\n",
        "    assert len(Alpha_bar) == T\n",
        "    assert 0.0 <= kappa <= 1.0\n",
        "\n",
        "    # compute diffusion hyperparameters for user defined noise\n",
        "    T_user = len(user_defined_eta)\n",
        "    Beta_tilde = torch.from_numpy(user_defined_eta).to(torch.float32)\n",
        "    Gamma_bar = 1 - Beta_tilde\n",
        "    for t in range(1, T_user):\n",
        "        Gamma_bar[t] *= Gamma_bar[t-1]\n",
        "\n",
        "    assert Gamma_bar[0] <= Alpha_bar[0] and Gamma_bar[-1] >= Alpha_bar[-1]\n",
        "\n",
        "    x_prev_multiplier = torch.zeros(T_user)\n",
        "    theta_multiplier = torch.zeros(T_user)\n",
        "    std = torch.zeros(T_user)\n",
        "    diffusion_steps_list = torch.zeros(T_user)\n",
        "\n",
        "\n",
        "    for i, tau in enumerate(continuous_steps):\n",
        "        diffusion_steps_list[i] = tau\n",
        "        if i == T_user - 1:  # the next step is to generate x_0\n",
        "            # assert abs(tau) < 0.1\n",
        "            alpha_next = torch.tensor(1.0)\n",
        "            sigma = torch.tensor(0.0)\n",
        "        else:\n",
        "            alpha_next = Gamma_bar[T_user-1-i - 1]\n",
        "            sigma = kappa * torch.sqrt((1-alpha_next) / (1-Gamma_bar[T_user-1-i]) * (1 - Gamma_bar[T_user-1-i] / alpha_next))\n",
        "        x_prev_multiplier[i] = torch.sqrt(alpha_next / Gamma_bar[T_user-1-i])\n",
        "        theta_multiplier[i] = torch.sqrt(1 - alpha_next - sigma ** 2) - torch.sqrt(1 - Gamma_bar[T_user-1-i]) * torch.sqrt(alpha_next / Gamma_bar[T_user-1-i])\n",
        "        if i == T_user - 1:\n",
        "            std[i] = 0.001\n",
        "        else:\n",
        "            std[i] = sigma\n",
        "\n",
        "    # return map_gpu(x_prev_multiplier), map_gpu(theta_multiplier), map_gpu(std), map_gpu(diffusion_steps_list)\n",
        "    return x_prev_multiplier, theta_multiplier, std, diffusion_steps_list\n",
        "\n",
        "\n",
        "def VAR_log_prob(net, x_prev, x_next, t, x_prev_multiplier, theta_multiplier, std, diffusion_steps_list):\n",
        "    # net.eval()\n",
        "    # net.train()\n",
        "    diffusion_steps = diffusion_steps_list[t] # shape ([bs])\n",
        "    epsilon_theta = net(x_prev, diffusion_steps)\n",
        "    # epsilon_theta_seq = net(torch.cat(x_seq[:10]), diffusion_steps)\n",
        "    pred_mean = x_prev*unsqueeze3x(x_prev_multiplier[t]) + unsqueeze3x(theta_multiplier[t])*epsilon_theta\n",
        "    pred_std = unsqueeze3x(std[t])\n",
        "    dist = Normal(pred_mean, pred_std)\n",
        "    log_prob = dist.log_prob(x_next.detach()).mean(dim = -1).mean(dim = -1).mean(dim = -1)\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "\n",
        "def VAR_sampling(net, size, diffusion_hyperparams, user_defined_eta,\n",
        "                    kappa, continuous_steps, device, trainable_beta = False,\n",
        "                    enable_grad=False, adhoc_scale1=1):\n",
        "    \"\"\"\n",
        "    Copy for not breaking other functions..\n",
        "\n",
        "    Perform the complete sampling step according to user defined variances\n",
        "\n",
        "    Parameters:\n",
        "    net (torch network):            the model\n",
        "    size (tuple):                   size of tensor to be generated,\n",
        "                                    usually is (number of audios to generate, channels=1, length of audio)\n",
        "    diffusion_hyperparams (dict):   dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams\n",
        "                                    note, the tensors need to be cuda tensors\n",
        "    user_defined_eta (np.array):    User defined noise\n",
        "    kappa (float):                  factor multipled over sigma, between 0 and 1\n",
        "    continuous_steps (list):        continuous steps computed from user_defined_eta\n",
        "\n",
        "    Returns:\n",
        "    the generated images in torch.tensor, shape=size\n",
        "    \"\"\"\n",
        "    # net.eval()\n",
        "    _dh = diffusion_hyperparams\n",
        "    T, Alpha, Alpha_bar, Beta = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Beta\"]\n",
        "    assert len(Alpha_bar) == T\n",
        "    assert len(size) == 4\n",
        "    assert 0.0 <= kappa <= 1.0\n",
        "\n",
        "    # compute diffusion hyperparameters for user defined noise\n",
        "    T_user = len(user_defined_eta)\n",
        "    Beta_tilde = torch.from_numpy(user_defined_eta).to(torch.float32)\n",
        "    Gamma_bar = 1 - Beta_tilde\n",
        "    for t in range(1, T_user):\n",
        "        Gamma_bar[t] *= Gamma_bar[t-1]\n",
        "\n",
        "    assert Gamma_bar[0] <= Alpha_bar[0] and Gamma_bar[-1] >= Alpha_bar[-1]\n",
        "\n",
        "\n",
        "    x = torch.randn(size, device=device)\n",
        "    x_seq = [x.detach().clone()]\n",
        "    log_prob_list = []\n",
        "    control_list = []\n",
        "    pred_mean_list = []\n",
        "    pred_std_list = []\n",
        "\n",
        "    with torch.set_grad_enabled(enable_grad):\n",
        "        for i, tau in enumerate(continuous_steps):\n",
        "            diffusion_steps = tau * (torch.ones(size[0], device=device))\n",
        "\n",
        "            epsilon_theta = net(x, diffusion_steps)\n",
        "\n",
        "            if i == T_user - 1:  # the next step is to generate x_0\n",
        "                # assert abs(tau) < 0.1\n",
        "                alpha_next = torch.tensor(1.0)\n",
        "                sigma = torch.tensor(0.0)\n",
        "            else:\n",
        "                alpha_next = Gamma_bar[T_user-1-i - 1]\n",
        "                sigma = kappa * torch.sqrt((1-alpha_next) / (1-Gamma_bar[T_user-1-i]) * (1 - Gamma_bar[T_user-1-i] / alpha_next))\n",
        "            x *= torch.sqrt(alpha_next / Gamma_bar[T_user-1-i]) # x_prev multiplier\n",
        "            c = torch.sqrt(1 - alpha_next - sigma ** 2) - torch.sqrt(1 - Gamma_bar[T_user-1-i]) * torch.sqrt(alpha_next / Gamma_bar[T_user-1-i]) # theta multiplier\n",
        "            control = c * epsilon_theta * adhoc_scale1\n",
        "            # Reshape control to match the shape of x\n",
        "            # control = control.reshape(x.shape)\n",
        "            pred_mean = x + control\n",
        "\n",
        "            # recompute beta\n",
        "            if trainable_beta:\n",
        "                if trainable_beta == 'fix_last':\n",
        "                    if hasattr(net, 'module'):\n",
        "                        log_betas_all = torch.cat([net.module.log_betas[:-1], net.module.std[-1].log().unsqueeze(0)])\n",
        "                    else:\n",
        "                        log_betas_all = torch.cat([net.log_betas[:-1], net.std[-1].log().unsqueeze(0)])\n",
        "                    sigma = torch.exp(log_betas_all[i])\n",
        "\n",
        "                else:\n",
        "                    if hasattr(net, 'module'):\n",
        "                        sigma = torch.exp(net.module.log_betas[i])\n",
        "                    else:\n",
        "                        sigma = torch.exp(net.log_betas[i])\n",
        "            else:\n",
        "                if i == T_user - 1:\n",
        "                    sigma = torch.tensor(0.001).to(device)\n",
        "\n",
        "            x += control + sigma * torch.randn(size, device=device)\n",
        "\n",
        "            pred_std = (unsqueeze3x(sigma.repeat(len(x))).to(device))\n",
        "            dist = Normal(pred_mean, pred_std)\n",
        "            log_prob = dist.log_prob(x.detach().clone()).mean(dim = -1).mean(dim = -1).mean(dim = -1)\n",
        "\n",
        "            x_seq.append(x.detach().clone())\n",
        "            log_prob_list.append(log_prob)\n",
        "            control_list.append(control.detach().clone())\n",
        "            pred_mean_list.append(pred_mean.detach().clone())\n",
        "            pred_std_list.append(pred_std.detach().clone())\n",
        "\n",
        "    return x_seq, log_prob_list, control_list, pred_mean_list, pred_std_list\n",
        "\n",
        "\n",
        "class VARSampler(nn.Module):\n",
        "    def __init__(self, net, n_timesteps, sample_shape,\n",
        "            trainable_beta=True, adhoc_scale1=1., adhoc_scale2=1.):\n",
        "        \"\"\"\n",
        "        trainable_beta: Bool or String. trainable diffusion coefficient (noise)\n",
        "        adhoc_scale: adhoc scaling of theta_multiplier and sigma. used in T=4. default is 1.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.n_timesteps = n_timesteps  # corresponds to T_user\n",
        "        self.sample_shape = sample_shape\n",
        "        self.adhoc_scale1 = adhoc_scale1\n",
        "        self.adhoc_scale2 = adhoc_scale2\n",
        "        self.trainable_beta = trainable_beta\n",
        "        assert trainable_beta in {True, False, 'fix_last'}\n",
        "        self.init_schedule()\n",
        "\n",
        "        x_prev_multiplier, theta_multiplier, std, diffusion_steps_list = VAR_get_params(\n",
        "                self.diffusion_hyperparams, self.user_defined_eta, self.kappa, self.continuous_steps)\n",
        "        self.register_buffer(\"x_prev_multiplier\", x_prev_multiplier)\n",
        "        self.register_buffer(\"theta_multiplier\", theta_multiplier)\n",
        "        self.register_buffer(\"std\", std)\n",
        "        self.register_buffer(\"diffusion_steps_list\", diffusion_steps_list)\n",
        "        if self.trainable_beta == 'fix_last':\n",
        "            self.net.register_buffer(\"std\", std)\n",
        "\n",
        "    def init_schedule(self):\n",
        "        schedule = 'quadratic'\n",
        "        diffusion_hyperparams = calc_diffusion_hyperparams(**diffusion_config)\n",
        "        self.diffusion_hyperparams = diffusion_hyperparams\n",
        "        self.kappa = 1.0\n",
        "        self.user_defined_eta = get_VAR_noise(self.n_timesteps, schedule)\n",
        "        continuous_steps = torch.tensor(_precompute_VAR_steps(diffusion_hyperparams, self.user_defined_eta))\n",
        "        self.register_buffer(\"continuous_steps\", continuous_steps)\n",
        "\n",
        "        Alpha_bar = diffusion_hyperparams[\"Alpha_bar\"]\n",
        "        Beta_tilde = torch.from_numpy(self.user_defined_eta).to(torch.float32)\n",
        "        Gamma_bar = 1 - Beta_tilde\n",
        "        for t in range(1, self.n_timesteps):\n",
        "            Gamma_bar[t] *= Gamma_bar[t-1]\n",
        "\n",
        "        assert Gamma_bar[0] <= Alpha_bar[0] and Gamma_bar[-1] >= Alpha_bar[-1]\n",
        "        self.register_buffer(\"Gamma_bar\", Gamma_bar)\n",
        "\n",
        "        l_sigma = []\n",
        "        for t in range(self.n_timesteps):\n",
        "            if t == self.n_timesteps - 1:\n",
        "                sigma = torch.tensor(0.001)\n",
        "            else:\n",
        "                alpha_next = self.Gamma_bar[self.n_timesteps-1-t - 1]\n",
        "                sigma = self.kappa * torch.sqrt((1-alpha_next) / (1-self.Gamma_bar[self.n_timesteps-1-t]) * (1 - self.Gamma_bar[self.n_timesteps-1-t] / alpha_next))\n",
        "            l_sigma.append(sigma)\n",
        "        sigmas = torch.stack(l_sigma, dim=0)\n",
        "\n",
        "        if self.trainable_beta:\n",
        "            self.net.log_betas = nn.Parameter(torch.log(sigmas * self.adhoc_scale2))  # it's actually log sigma\n",
        "\n",
        "    def sample_step(self, x, t, y=None):\n",
        "        \"\"\"\n",
        "        assume t is a scalar\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        t = process_single_t(x, t)  # t is now 1D tensor\n",
        "        diffusion_steps = self.continuous_steps[t]\n",
        "        is_last_t = t == self.n_timesteps - 1\n",
        "\n",
        "        epsilon_theta = self.net(x, diffusion_steps)\n",
        "        alpha_next = self.Gamma_bar[self.n_timesteps-1-t - 1]\n",
        "        alpha_next = alpha_next * (~is_last_t) + (is_last_t) * 1.0\n",
        "        sigma = self.kappa * torch.sqrt((1-alpha_next) / (1-self.Gamma_bar[self.n_timesteps-1-t]) * (1 - self.Gamma_bar[self.n_timesteps-1-t] / alpha_next))\n",
        "        sigma = sigma * (~is_last_t) + (is_last_t) * 0  # last step is assigned later\n",
        "        sigma = sigma.to(device)\n",
        "\n",
        "        x_mult = torch.sqrt(alpha_next / self.Gamma_bar[self.n_timesteps-1-t]) # x_prev multiplier\n",
        "        x_mult = x_mult.unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
        "        x = x * x_mult\n",
        "        c = torch.sqrt(1 - alpha_next - sigma ** 2) - torch.sqrt(1 - self.Gamma_bar[self.n_timesteps-1-t]) * torch.sqrt(alpha_next / self.Gamma_bar[self.n_timesteps-1-t]) # theta multiplier\n",
        "        c = c.unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
        "        control = c * epsilon_theta * self.adhoc_scale1\n",
        "\n",
        "        pred_mean = x + control\n",
        "\n",
        "        # recompute sigma\n",
        "        if self.trainable_beta:\n",
        "            if self.trainable_beta == 'fix_last':\n",
        "                if hasattr(self.net, 'module'):\n",
        "                    log_betas_all = torch.cat([self.net.module.log_betas[:-1], self.net.module.std[-1].log().unsqueeze(0)])\n",
        "                else:\n",
        "                    log_betas_all = torch.cat([self.net.log_betas[:-1], self.net.std[-1].log().unsqueeze(0)])\n",
        "                sigma = torch.exp(log_betas_all[t])\n",
        "            else:\n",
        "                if hasattr(self.net, 'module'):\n",
        "                    sigma = torch.exp(self.net.module.log_betas[t])\n",
        "                else:\n",
        "                    sigma = torch.exp(self.net.log_betas[t])\n",
        "        else:\n",
        "            sigma = sigma * (~is_last_t) + (is_last_t) * 0.001\n",
        "        sigma = sigma.unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        x = pred_mean + sigma * torch.randn_like(x)\n",
        "\n",
        "        dist = Normal(pred_mean, sigma)\n",
        "        log_prob = dist.log_prob(x.detach().clone()).mean(-1).mean(-1).mean(-1)\n",
        "\n",
        "        entropy = torch.log(sigma)  # normalize by dimensionality for convenience\n",
        "        d_step = {\"sample\": x, \"logp\": log_prob, \"logp_terminal\": torch.zeros(len(x), device=device),\n",
        "                  \"mean\": pred_mean, \"sigma\": sigma,\n",
        "                  \"entropy\": entropy, \"control\": control}\n",
        "        return d_step\n",
        "\n",
        "\n",
        "    def sample(self, n_sample, device=\"cuda\", enable_grad=False):\n",
        "        size = (n_sample, *self.sample_shape)\n",
        "        samples, log_prob_list, control_list, pred_mean_list, pred_std_list = VAR_sampling(self.net, size,\n",
        "                self.diffusion_hyperparams, self.user_defined_eta,\n",
        "                self.kappa, self.continuous_steps, device=device,\n",
        "                trainable_beta=self.trainable_beta, enable_grad=enable_grad,\n",
        "                adhoc_scale1=self.adhoc_scale1)\n",
        "        x = samples[-1]\n",
        "\n",
        "        logp_terminal = torch.zeros(len(x), device=device)\n",
        "        d_sample = {'sample': x,\n",
        "                    'l_sample': samples,\n",
        "                    'logp': log_prob_list,\n",
        "                    'logp_terminal': logp_terminal,\n",
        "                    'mean': pred_mean_list,\n",
        "                    'sigma': pred_std_list,\n",
        "                    'control': control_list}\n",
        "        return d_sample\n",
        "\n",
        "\n",
        "    def log_prob_step(self, x_prev, x_next, t):\n",
        "\n",
        "        x_prev_multiplier, theta_multiplier, std, diffusion_steps_list = VAR_get_params(\n",
        "                self.diffusion_hyperparams, self.user_defined_eta, self.kappa, self.continuous_steps)\n",
        "        device = x_prev.device\n",
        "        x_prev_multiplier = x_prev_multiplier.to(device)\n",
        "        theta_multiplier = theta_multiplier.to(device)\n",
        "        diffusion_steps_list = diffusion_steps_list.to(device)\n",
        "        std = std.to(device)\n",
        "\n",
        "        log_prob = VAR_log_prob(self.net, x_prev, x_next, t,\n",
        "                x_prev_multiplier, theta_multiplier,\n",
        "                std, diffusion_steps_list)\n",
        "        return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "5r5Hxzzne_7t",
        "outputId": "93fd9d2a-ab92-4098-e4f4-56d16340fa99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# n_reduced_timesteps = 5\n",
        "\n",
        "sampler = VARSampler(policy_net, n_timesteps=n_reduced_timesteps, sample_shape=data_size, trainable_beta=False)\n",
        "sampler = sampler.to(device)\n",
        "\n",
        "sampler.eval()\n",
        "d_sample = sampler.sample(10,device=device)\n",
        "samples = d_sample['sample']\n",
        "\n",
        "# Set up a 2x5 grid for 10 images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, ax in enumerate(axes):\n",
        "    img = samples[idx]\n",
        "    # Clip the image tensor so values are within [0, 1]\n",
        "    img = torch.clamp(img, -1, 1)\n",
        "    img = ((img + 1) / 2 * 255).to(torch.uint8)\n",
        "\n",
        "    # Permute dimensions from [C, H, W] to [H, W, C] for matplotlib\n",
        "    img = img.permute(1, 2, 0)\n",
        "    ax.imshow(img.cpu().numpy(), cmap='gray')\n",
        "    ax.axis('off')  # Remove axes ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'var_sampler_images_{now_str}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjh85NAwpOWF",
        "outputId": "fbb266ed-65c2-42b4-8cc5-e7840e39e4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downsample FID: 2.240639\n",
            "original FID: 1.288106\n"
          ]
        }
      ],
      "source": [
        "score = compute_fid(None, None, device, sample_size=1000, sampler=sampler)\n",
        "print(f\"downsample FID: {score:.6f}\")\n",
        "\n",
        "score = compute_fid(policy_net, diffusion, device, sample_size=1000, sampler=None)\n",
        "print(f\"original FID: {score:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lu-TkJ7s2Dxt",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def append_buffer(state_buffer, d_sample):\n",
        "    x_seq = d_sample[\"l_sample\"]\n",
        "    n_sample = len(x_seq[0])\n",
        "    n_seq = len(x_seq) - 1\n",
        "    device = x_seq[0].device\n",
        "\n",
        "    for t in range(n_seq):\n",
        "        state_buffer[\"state\"] = torch.cat((state_buffer[\"state\"], x_seq[t].detach()))\n",
        "        state_buffer[\"next_state\"] = torch.cat(\n",
        "            (state_buffer[\"next_state\"], x_seq[t + 1].detach())\n",
        "        )\n",
        "        state_buffer[\"timestep\"] = torch.cat(\n",
        "            (state_buffer[\"timestep\"], torch.tensor([t] * n_sample).to(device))\n",
        "        )\n",
        "        state_buffer[\"final\"] = torch.cat((state_buffer[\"final\"], x_seq[-1].detach()))\n",
        "        if 'logp' in d_sample:\n",
        "            logp = d_sample[\"logp\"]\n",
        "            state_buffer[\"logp\"] = torch.cat((state_buffer[\"logp\"], logp[t].detach()))\n",
        "        if 'control' in d_sample:\n",
        "            state_buffer[\"control\"] = torch.cat((state_buffer[\"control\"],\n",
        "                                                 d_sample['control'][t].detach()))\n",
        "        if 'entropy' in d_sample:\n",
        "            state_buffer[\"entropy\"] = torch.cat((state_buffer[\"entropy\"],\n",
        "                                                 d_sample['entropy'][t].detach()))\n",
        "        if 'mean' in d_sample:\n",
        "            state_buffer[\"mean\"] = torch.cat((state_buffer[\"mean\"],\n",
        "                                                 d_sample['mean'][t].detach()))\n",
        "        if 'sigma' in d_sample:\n",
        "            state_buffer[\"sigma\"] = torch.cat((state_buffer[\"sigma\"],\n",
        "                                                 d_sample['sigma'][t].detach()))\n",
        "        if 'y' in d_sample:\n",
        "            state_buffer[\"y\"] = torch.cat((state_buffer[\"y\"], d_sample[\"y\"].detach()))\n",
        "    return state_buffer\n",
        "\n",
        "\n",
        "def reset_buffer(device):\n",
        "    state_dict = {}\n",
        "    state_dict['state'] = torch.FloatTensor().to(device)\n",
        "    state_dict['next_state'] = torch.FloatTensor().to(device)\n",
        "    state_dict['timestep'] = torch.LongTensor().to(device)\n",
        "    state_dict['final'] = torch.FloatTensor().to(device)\n",
        "    state_dict['logp'] = torch.FloatTensor().to(device)\n",
        "    state_dict['control'] = torch.FloatTensor().to(device)\n",
        "    state_dict['entropy'] = torch.FloatTensor().to(device)\n",
        "    state_dict['mean'] = torch.FloatTensor().to(device)\n",
        "    state_dict['sigma'] = torch.FloatTensor().to(device)\n",
        "    state_dict[\"y\"] = torch.LongTensor().to(device)\n",
        "    return state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JDXbH3JvlNho"
      },
      "outputs": [],
      "source": [
        "# hidden_size_value = 512\n",
        "# hidden_layers_value = 5\n",
        "\n",
        "value_net = ValueNet(hidden_size=hidden_size_value, hidden_layers=hidden_layers_value, data_size=data_size).to(device)\n",
        "\n",
        "# create target critic\n",
        "import copy\n",
        "value_net_target = copy.deepcopy(value_net)\n",
        "# freeze target's gradients\n",
        "for p in value_net_target.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DDhn7xb2AxVf"
      },
      "outputs": [],
      "source": [
        "# learning_rate_policy = 1e-7\n",
        "# learning_rate_value = 1e-6\n",
        "\n",
        "optimizer_value  = optim.Adam(value_net.parameters(), lr=learning_rate_value)\n",
        "optimizer_policy = optim.Adam(policy_net.parameters(), lr=learning_rate_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kC5p-U3UHWV8"
      },
      "outputs": [],
      "source": [
        "def save_value_checkpoint(policy_net, value_net, epoch):\n",
        "    checkpoint = {\n",
        "        \"policy_net_state_dict\": policy_net.state_dict(),\n",
        "        \"diffusion_state_dict\": diffusion.state_dict(),\n",
        "        \"hyperparameters\": {\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"embedding_size\": embedding_size,\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"hidden_layers\": hidden_layers,\n",
        "            \"data_size\": data_size,\n",
        "            \"limit_samples\": limit_samples,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"train_batch_size\": train_batch_size,\n",
        "            \"num_epochs\": num_epochs,\n",
        "            \"device\": device.type\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save the checkpoint to a file.\n",
        "    save_path = f\"policy_net_during_dxmi_e{epoch:05}_{now_str}.pt\"\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "    checkpoint = {\n",
        "        \"value_net_state_dict\": policy_net.state_dict(),\n",
        "        \"hyperparameters\": {\n",
        "            \"n_reduced_timesteps\": n_reduced_timesteps,\n",
        "            \"hidden_size_value\": hidden_size_value,\n",
        "            \"hidden_layers_value\": hidden_layers_value,\n",
        "            \"learning_rate_value\": learning_rate_value,\n",
        "            \"learning_rate_policy\": learning_rate_policy,\n",
        "            \"num_epochs\": num_epochs,\n",
        "            \"data_size\": data_size\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save the checkpoint to a file.\n",
        "    save_path = f\"value_net_during_e{epoch:05}_{now_str}.pt\"\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TseUZsGjP2hG"
      },
      "outputs": [],
      "source": [
        "def soft_update(online_net, target_net, tau=0.005):\n",
        "    \"\"\"\n",
        "    Perform a soft (Polyak) update of target_net’s parameters toward online_net’s.\n",
        "\n",
        "    Args:\n",
        "      online_net (torch.nn.Module): the live network (e.g. value_net)\n",
        "      target_net (torch.nn.Module): the target network (e.g. value_net_target)\n",
        "      tau (float): interpolation factor in [0,1], e.g. 0.005\n",
        "    \"\"\"\n",
        "    for param, param_target in zip(online_net.parameters(),\n",
        "                                   target_net.parameters()):\n",
        "        # target ← (1 - tau) * target + tau * online\n",
        "        param_target.data.mul_(1.0 - tau)\n",
        "        param_target.data.add_(tau * param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P-nNx0_7PeAF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_running_cost(state, next_state, pred_mean, pred_std, eps: float = 1e-8):\n",
        "    \"\"\"\n",
        "    Stateless running‐cost: ½‖u‖²  where  u = (next_state - pred_mean) / pred_std.\n",
        "    Returns a tensor of shape (B,) giving the mean‐per‐sample cost.\n",
        "    \"\"\"\n",
        "    u = (next_state - pred_mean) / (pred_std + eps)\n",
        "    return 0.5 * u.view(u.size(0), -1).pow(2).mean(dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP-TSAdXi4eM",
        "outputId": "dccb6ffb-223a-4fd1-bc09-a2e50dcc6ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 took 8.00s, Energy Loss: -0.300601, Value Loss: 1.933304, Policy Loss: 4.365587, Entropy: -2.942267, Running Cost: 0.500162, Positive Energy: 4.512475, Negative Energy: 4.813076, FID: 2.596737\n",
            "Epoch 2/10 took 8.37s, Energy Loss: -0.276359, Value Loss: 5.185423, Policy Loss: 8.872406, Entropy: -2.935073, Running Cost: 0.500491, Positive Energy: 9.187997, Negative Energy: 9.464356, FID: 2.339355\n",
            "Epoch 3/10 took 7.89s, Energy Loss: -0.186854, Value Loss: 4.954916, Policy Loss: 10.510052, Entropy: -2.952008, Running Cost: 0.499737, Positive Energy: 10.733221, Negative Energy: 10.920075, FID: 2.746664\n",
            "Epoch 4/10 took 7.93s, Energy Loss: -0.141989, Value Loss: 3.521940, Policy Loss: 11.413683, Entropy: -2.947683, Running Cost: 0.499237, Positive Energy: 11.569769, Negative Energy: 11.711758, FID: 2.785339\n",
            "Epoch 5/10 took 7.38s, Energy Loss: -0.118377, Value Loss: 2.227380, Policy Loss: 12.040941, Entropy: -2.978539, Running Cost: 0.500102, Positive Energy: 12.146550, Negative Energy: 12.264926, FID: 3.095757\n",
            "Epoch 6/10 took 8.29s, Energy Loss: -0.101812, Value Loss: 1.362769, Policy Loss: 12.519856, Entropy: -2.952732, Running Cost: 0.499554, Positive Energy: 12.595494, Negative Energy: 12.697306, FID: 2.515560\n",
            "Epoch 7/10 took 7.90s, Energy Loss: -0.089380, Value Loss: 0.842453, Policy Loss: 12.915835, Entropy: -2.914879, Running Cost: 0.500343, Positive Energy: 12.971879, Negative Energy: 13.061259, FID: 2.763113\n",
            "Epoch 8/10 took 8.28s, Energy Loss: -0.080662, Value Loss: 0.546235, Policy Loss: 13.273082, Entropy: -2.922508, Running Cost: 0.498888, Positive Energy: 13.304372, Negative Energy: 13.385034, FID: 2.588438\n",
            "Epoch 9/10 took 7.39s, Energy Loss: -0.074638, Value Loss: 0.350244, Policy Loss: 13.595713, Entropy: -2.962440, Running Cost: 0.499466, Positive Energy: 13.607987, Negative Energy: 13.682625, FID: 2.762625\n",
            "Epoch 10/10 took 9.33s, Energy Loss: -0.069585, Value Loss: 0.228977, Policy Loss: 13.894915, Entropy: -2.952864, Running Cost: 0.500029, Positive Energy: 13.894161, Negative Energy: 13.963747, FID: 2.744268\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "# --- Hyperparameters & logging setup ---\n",
        "value_iteration_file = f\"value_iteration_{now_str}.txt\"\n",
        "\n",
        "# num_epochs          = 1000\n",
        "# target_network      = False\n",
        "# entropy_coef        = 0.0   # τ₁\n",
        "# cost_coef           = 0.1   # τ₂\n",
        "# gradient_clipping   = 0.0\n",
        "\n",
        "num_epochs        = dxmi_num_epochs\n",
        "target_network    = dxmi_target_network\n",
        "entropy_coef      = dxmi_entropy_coef\n",
        "cost_coef         = dxmi_cost_coef\n",
        "gradient_clipping = dxmi_gradient_clipping\n",
        "\n",
        "hyperparams = {\n",
        "    \"n_reduced_timesteps\": n_reduced_timesteps,\n",
        "    \"hidden_size_value\":   hidden_size_value,\n",
        "    \"hidden_layers_value\":  hidden_layers_value,\n",
        "    \"learning_rate_value\":  learning_rate_value,\n",
        "    \"learning_rate_policy\": learning_rate_policy,\n",
        "    \"num_epochs\":           num_epochs,\n",
        "    \"target_network\":       target_network,\n",
        "    \"entropy_coef\":         entropy_coef,\n",
        "    \"cost_coef\":            cost_coef,\n",
        "    \"gradient_clipping\": gradient_clipping,\n",
        "}\n",
        "\n",
        "with open(log_file, \"a\") as f:\n",
        "    f.write(\"=== Hyperparameters for value iteration ===\\n\")\n",
        "    for name, val in hyperparams.items():\n",
        "        f.write(f\"{name}: {val}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    f.flush()\n",
        "\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    policy_net.train(); value_net.train()\n",
        "\n",
        "    # metrics accumulators\n",
        "    epoch_metrics = {\n",
        "        \"energy_loss\":     0.0,\n",
        "        \"positive_energy\": 0.0,\n",
        "        \"negative_energy\": 0.0,\n",
        "        \"value_loss\":      0.0,\n",
        "        \"policy_loss\":     0.0,\n",
        "        \"entropy\":         0.0,\n",
        "        \"running_cost\":    0.0,\n",
        "    }\n",
        "    updates = {\"energy\": 0, \"value\": 0, \"policy\": 0}\n",
        "\n",
        "    # refill replay buffer\n",
        "    state_dict = reset_buffer(device)\n",
        "\n",
        "    for (images,) in train_loader:\n",
        "        # --- Energy update ---\n",
        "        sampler.eval()\n",
        "        images = images.to(device)\n",
        "        d_sample = sampler.sample(len(images), device=device)\n",
        "        append_buffer(state_dict, d_sample)\n",
        "\n",
        "        x_seq = d_sample[\"l_sample\"]\n",
        "        x0 = x_seq[-1]\n",
        "        inputs = torch.cat((images.detach(), x0.detach()), dim=0)\n",
        "        out = value_net(inputs)\n",
        "        pos_e, neg_e = out[:x0.shape[0]], out[x0.shape[0]:]\n",
        "\n",
        "        d_loss = pos_e.mean() - neg_e.mean()\n",
        "        optimizer_value.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_value.step()\n",
        "\n",
        "        epoch_metrics[\"energy_loss\"]     += d_loss.item()\n",
        "        epoch_metrics[\"positive_energy\"] += pos_e.mean().item()\n",
        "        epoch_metrics[\"negative_energy\"] += neg_e.mean().item()\n",
        "        updates[\"energy\"] += 1\n",
        "\n",
        "        # --- Value update (TD) ---\n",
        "        n_steps   = n_reduced_timesteps\n",
        "        batchsize = train_batch_size\n",
        "        offset    = state_dict[\"state\"].shape[0] - batchsize * n_steps\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            start = (n_steps - i - 1) * batchsize + offset\n",
        "            end   = (n_steps - i)     * batchsize + offset\n",
        "            s  = state_dict[\"state\"][start:end]\n",
        "            ns = state_dict[\"next_state\"][start:end]\n",
        "\n",
        "            if target_network:\n",
        "                # Use the target for TD bootstrapping\n",
        "                with torch.no_grad():\n",
        "                    tgt = value_net_target(ns).squeeze()\n",
        "            else:\n",
        "                tgt = value_net(ns).squeeze()\n",
        "\n",
        "            v_xt   = value_net(s).squeeze()\n",
        "            v_loss = F.mse_loss(v_xt, tgt.detach())\n",
        "\n",
        "            optimizer_value.zero_grad()\n",
        "            v_loss.backward()\n",
        "            optimizer_value.step()\n",
        "\n",
        "            epoch_metrics[\"value_loss\"] += v_loss.item()\n",
        "            updates[\"value\"] += 1\n",
        "\n",
        "            if target_network:\n",
        "                # update target\n",
        "                soft_update(value_net, value_net_target, tau=0.005)\n",
        "\n",
        "\n",
        "        # --- Policy update (with entropy & running cost) ---\n",
        "        sampler.train()\n",
        "        permutation = torch.randperm(state_dict[\"state\"].shape[0])\n",
        "        n_data      = min(len(permutation), batchsize)\n",
        "\n",
        "        for m in range(0, n_data, batchsize):\n",
        "            idx = permutation[m : m + batchsize]\n",
        "            s = state_dict[\"state\"][idx]\n",
        "            t = state_dict[\"timestep\"][idx]\n",
        "\n",
        "            dss = sampler.sample_step(s, t)\n",
        "            ns  = dss[\"sample\"]\n",
        "            μ   = dss[\"mean\"]\n",
        "            σ   = dss[\"sigma\"]\n",
        "\n",
        "            # value-based part\n",
        "            pvl = value_net(ns).mean()\n",
        "\n",
        "            # running cost\n",
        "            rc = get_running_cost(s, ns, μ, σ)\n",
        "\n",
        "            # This is often done because you don’t want to\n",
        "            # penalize control effort or entropy at the final step when you’re “finishing” the trajectory.\n",
        "            non_term = (t < n_reduced_timesteps - 1).float()\n",
        "\n",
        "            # entropy bonus\n",
        "            ent = σ.squeeze().log()\n",
        "\n",
        "            # combined loss\n",
        "            loss = (\n",
        "                pvl\n",
        "                + cost_coef    * rc  * non_term\n",
        "                - entropy_coef * ent * non_term\n",
        "            ).mean()\n",
        "\n",
        "            optimizer_policy.zero_grad()\n",
        "            loss.backward()\n",
        "            if gradient_clipping > 0.0:\n",
        "                total_norm = torch.nn.utils.clip_grad_norm_(sampler.parameters(), gradient_clipping)\n",
        "            optimizer_policy.step()\n",
        "\n",
        "            epoch_metrics[\"policy_loss\"]  += loss.item()\n",
        "            epoch_metrics[\"entropy\"]      += ent.mean().item()\n",
        "            epoch_metrics[\"running_cost\"] += rc.mean().item()\n",
        "            updates[\"policy\"] += 1\n",
        "\n",
        "    # --- Compute averages & log ---\n",
        "    avg = {\n",
        "        \"energy_loss\":     epoch_metrics[\"energy_loss\"]     / updates[\"energy\"],\n",
        "        \"positive_energy\": epoch_metrics[\"positive_energy\"] / updates[\"energy\"],\n",
        "        \"negative_energy\": epoch_metrics[\"negative_energy\"] / updates[\"energy\"],\n",
        "        \"value_loss\":      epoch_metrics[\"value_loss\"]      / updates[\"value\"],\n",
        "        \"policy_loss\":     epoch_metrics[\"policy_loss\"]     / updates[\"policy\"],\n",
        "        \"entropy\":         epoch_metrics[\"entropy\"]         / updates[\"policy\"],\n",
        "        \"running_cost\":    epoch_metrics[\"running_cost\"]    / updates[\"policy\"],\n",
        "    }\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    fid_score  = compute_fid(None, None, device=device, sampler=sampler)\n",
        "\n",
        "    output_line = (\n",
        "        f\"Epoch {epoch+1}/{num_epochs} took {total_time:.2f}s, \"\n",
        "        f\"Energy Loss: {avg['energy_loss']:.6f}, \"\n",
        "        f\"Value Loss: {avg['value_loss']:.6f}, \"\n",
        "        f\"Policy Loss: {avg['policy_loss']:.6f}, \"\n",
        "        f\"Entropy: {avg['entropy']:.6f}, \"\n",
        "        f\"Running Cost: {avg['running_cost']:.6f}, \"\n",
        "        f\"Positive Energy: {avg['positive_energy']:.6f}, \"\n",
        "        f\"Negative Energy: {avg['negative_energy']:.6f}, \"\n",
        "        f\"FID: {fid_score:.6f}\"\n",
        "    )\n",
        "    print(output_line)\n",
        "    with open(value_iteration_file, \"a\") as f:\n",
        "        f.write(output_line + \"\\n\")\n",
        "        f.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "MtDSCqsAX7uu",
        "outputId": "c02159f4-7fc5-401d-9005-0d00010a5adc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sampler.eval()\n",
        "d_sample = sampler.sample(10,device=device)\n",
        "\n",
        "samples = d_sample['sample']\n",
        "\n",
        "# Set up a 2x5 grid for 10 images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, ax in enumerate(axes):\n",
        "    img = samples[idx]\n",
        "    # Clip the image tensor so values are within [0, 1]\n",
        "    img = torch.clamp(img, -1, 1)\n",
        "    img = ((img + 1) / 2 * 255).to(torch.uint8)\n",
        "\n",
        "    # Permute dimensions from [C, H, W] to [H, W, C] for matplotlib\n",
        "    img = img.permute(1, 2, 0)\n",
        "    ax.imshow(img.cpu().numpy(), cmap='gray')\n",
        "    ax.axis('off')  # Remove axes ticks\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'value_iteration_images_{now_str}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2HtEil1HWV-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "dxmi [~/.conda/envs/dxmi/]",
      "language": "python",
      "name": "conda_dxmi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}